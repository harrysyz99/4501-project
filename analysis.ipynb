{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from typing import List, Union, Set\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[167], line 37\u001B[0m\n\u001B[0;32m     32\u001B[0m filenames \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/311_Service_Requests.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/2015 Street Tree Census.json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/Bedbug Reporting.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(urls)):\n\u001B[1;32m---> 37\u001B[0m     \u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43murls\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[167], line 19\u001B[0m, in \u001B[0;36mdownload\u001B[1;34m(url, filename)\u001B[0m\n\u001B[0;32m     17\u001B[0m count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Download the file in chunks\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m r\u001B[38;5;241m.\u001B[39miter_content(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size):\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# print(f\"{filename} has download {count} data\")\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(chunk)\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# count+=chunk_size\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py:624\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m    608\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    609\u001B[0m \u001B[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001B[39;00m\n\u001B[0;32m    610\u001B[0m \u001B[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;124;03m    'content-encoding' header.\u001B[39;00m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunked \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupports_chunked_reads():\n\u001B[1;32m--> 624\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_chunked(amt, decode_content\u001B[38;5;241m=\u001B[39mdecode_content):\n\u001B[0;32m    625\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m line\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py:832\u001B[0m, in \u001B[0;36mHTTPResponse.read_chunked\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m    830\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    831\u001B[0m chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_chunk(amt)\n\u001B[1;32m--> 832\u001B[0m decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    833\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflush_decoder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m    834\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoded:\n\u001B[0;32m    836\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m decoded\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py:407\u001B[0m, in \u001B[0;36mHTTPResponse._decode\u001B[1;34m(self, data, decode_content, flush_decoder)\u001B[0m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoder:\n\u001B[1;32m--> 407\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecompress\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDECODER_ERROR_CLASSES \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    409\u001B[0m     content_encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent-encoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mlower()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py:94\u001B[0m, in \u001B[0;36mGzipDecoder.decompress\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 94\u001B[0m         ret \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecompress\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m zlib\u001B[38;5;241m.\u001B[39merror:\n\u001B[0;32m     96\u001B[0m         previous_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "output_folder = \"data\"\n",
    "chunk_size = 102400\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def download(url, filename):\n",
    "    # Send an HTTP GET request to the specified URL\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if r.status_code == 200:\n",
    "        # Open the file in write-binary mode\n",
    "        with open(filename, 'ab') as f:\n",
    "            # Define the chunk size (e.g., 1024 bytes)\n",
    "\n",
    "            count = 0\n",
    "            # Download the file in chunks\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                # print(f\"{filename} has download {count} data\")\n",
    "                f.write(chunk)\n",
    "                # count+=chunk_size\n",
    "\n",
    "        print(\"\\nDownload complete.\")\n",
    "    else:\n",
    "        print(\"Error: Failed to download the file.\")\n",
    "\n",
    "urls = [\"https://data.cityofnewyork.us/api/views/erm2-nwe9/rows.csv?accessType=DOWNLOAD\",\n",
    "        \"https://data.cityofnewyork.us/resource/5rq2-4hqu.csv\",\n",
    "        \"https://data.cityofnewyork.us/resource/wz6d-d3jb.csv\"]\n",
    "\n",
    "filenames = [\"data/311_Service_Requests.csv\",\"data/2015 Street Tree Census.json\",\"data/Bedbug Reporting.csv\"]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(urls)):\n",
    "    download(urls[i],filenames[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T06:20:35.844021700Z",
     "start_time": "2023-12-08T06:16:42.976958300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_shapefile(shapefile_path: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Reads the shapefile into a GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "    - shapefile_path (str): Path to the shapefile.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: The GeoDataFrame read from the shapefile.\n",
    "    \"\"\"\n",
    "    return gpd.read_file(shapefile_path)\n",
    "\n",
    "def filter_columns(gdf: gpd.GeoDataFrame, columns: List[str]) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Filters the GeoDataFrame to include only specified columns.\n",
    "\n",
    "    Args:\n",
    "    - gdf (gpd.GeoDataFrame): The original GeoDataFrame.\n",
    "    - columns (List[str]): A list of column names to retain.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: The GeoDataFrame with only the specified columns.\n",
    "    \"\"\"\n",
    "    return gdf[columns]\n",
    "\n",
    "def remove_duplicates(gdf: gpd.GeoDataFrame, subset: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicate rows based on a specified subset of columns.\n",
    "\n",
    "    Args:\n",
    "    - gdf (gpd.GeoDataFrame): The GeoDataFrame to process.\n",
    "    - subset (str): Column name to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: The GeoDataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    return gdf.drop_duplicates(subset=[subset])\n",
    "\n",
    "def filter_invalid_zipcodes(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Keeps only rows with valid 5-digit zipcodes.\n",
    "\n",
    "    Args:\n",
    "    - gdf (gpd.GeoDataFrame): The GeoDataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: The GeoDataFrame with only valid 5-digit zipcodes.\n",
    "    \"\"\"\n",
    "    gdf['zipcode'] = gdf['zipcode'].astype(str)\n",
    "    return gdf[gdf['zipcode'].str.isdigit() & (gdf['zipcode'].str.len() == 5)]\n",
    "\n",
    "def process_zipcode_shapefile(shapefile_path: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and prepares a zipcode shapefile for analysis.\n",
    "\n",
    "    Args:\n",
    "    - shapefile_path (str): Path to the zipcode shapefile.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: GeoDataFrame with processed zipcode data.\n",
    "    \"\"\"\n",
    "    zipcode_gdf = read_shapefile(shapefile_path)\n",
    "    essential_columns = ['ZIPCODE', 'geometry']\n",
    "    zipcode_gdf = filter_columns(zipcode_gdf, essential_columns)\n",
    "    zipcode_gdf = remove_duplicates(zipcode_gdf, 'ZIPCODE')\n",
    "    zipcode_gdf.dropna(subset=essential_columns, inplace=True)\n",
    "    zipcode_gdf.rename(columns={'ZIPCODE': 'zipcode'}, inplace=True)\n",
    "    zipcode_gdf = filter_invalid_zipcodes(zipcode_gdf)\n",
    "    common_crs = \"EPSG:3857\"\n",
    "    zipcode_gdf.to_crs(common_crs, inplace=True)\n",
    "    zipcode_gdf.columns = map(str.lower, zipcode_gdf.columns)\n",
    "\n",
    "    return zipcode_gdf\n",
    "\n",
    "def lat_validation(latitude):\n",
    "    if not isinstance(latitude, (int, float)):\n",
    "        raise TypeError(\"The latitude should be a float or int type\")\n",
    "    return -90 <= latitude <= 90\n",
    "\n",
    "\n",
    "def long_validation(longitude: float) -> bool:\n",
    "    if not isinstance(longitude, (int, float)):\n",
    "        raise TypeError(\"The longitude should be a float or int type\")\n",
    "    return -180 <= longitude <= 180\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "geodf_zip_data = process_zipcode_shapefile(\"data/nyc_zipcodes/nyc_zipcodes.shp\")\n",
    "nyc_zips = geodf_zip_data['zipcode'].tolist()\n",
    "nyc_zips = [float(element) for element in nyc_zips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "columns_needed = ['Unique Key', 'Created Date', 'Complaint Type', 'Incident Zip', 'Latitude', 'Longitude', 'Location']\n",
    "def filter_t311(df: pd.DataFrame, column_needed: List[str], nyc_zip: Union[Set[str], List[str]]) -> gpd.GeoDataFrame:\n",
    "    # Filter the DataFrame to only include necessary columns and drop rows with NaN values\n",
    "    filtered = df[column_needed].dropna()\n",
    "\n",
    "    # Further filter the DataFrame to only include rows where 'Incident Zip' is in nyc_zip\n",
    "    filtered = filtered[filtered['Incident Zip'].isin(nyc_zip)]\n",
    "\n",
    "    # Converting 'Created Date' to datetime\n",
    "    filtered['Created Date'] = pd.to_datetime(filtered['Created Date'])\n",
    "\n",
    "    # Define your date range\n",
    "    start_date = pd.to_datetime('2015-01-01')\n",
    "    end_date = pd.to_datetime('2023-09-30')\n",
    "\n",
    "    # Filter the DataFrame for dates within the range\n",
    "    filtered = filtered[(filtered['Created Date'] >= start_date) & (filtered['Created Date'] <= end_date)]\n",
    "\n",
    "    # Apply latitude and longitude validation\n",
    "    filtered = filtered[filtered['Latitude'].apply(lat_validation) & filtered['Longitude'].apply(long_validation)]\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    filtered = gpd.GeoDataFrame(filtered, geometry=gpd.points_from_xy(filtered['Longitude'], filtered['Latitude']))\n",
    "    filtered.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    filtered.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## cehck\n",
    "def filter_stc(df: pd.DataFrame, column_needed: List[str], nyc_zip: Set[str]) -> gpd.GeoDataFrame:\n",
    "    # Filter the DataFrame to only include necessary columns and drop rows with NaN values\n",
    "    filtered = df[columns_needed].dropna()\n",
    "\n",
    "    # Further filter the DataFrame to only include rows where 'zipcode' is in nyc_zip\n",
    "    filtered = filtered[filtered['zipcode'].isin(nyc_zip)]\n",
    "\n",
    "    # Converting 'created_at' to datetime\n",
    "    filtered['created_at'] = pd.to_datetime(filtered['created_at'])\n",
    "\n",
    "    # Define your date range\n",
    "    start_date = pd.to_datetime('01/01/2015')\n",
    "    end_date = pd.to_datetime('09/30/2023')  # Corrected date\n",
    "\n",
    "    # Filter the DataFrame for dates within the range\n",
    "    filtered = filtered[(filtered['created_at'] >= start_date) & (filtered['created_at'] <= end_date)]\n",
    "\n",
    "    # Apply latitude and longitude validation\n",
    "    filtered = filtered[filtered['Latitude'].apply(lat_validation) & filtered['longitude'].apply(long_validation)]\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    filtered = gpd.GeoDataFrame(filtered, geometry=gpd.points_from_xy(filtered['longitude'], filtered['Latitude']))\n",
    "    filtered.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    filtered.to_crs(\"EPSG:3857\", inplace=True)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def filter_zillow(df: pd.DataFrame, nyc_zip: list) -> pd.DataFrame:\n",
    "    # Selecting the required columns. Assuming the first column is 'RegionName' and the 9th to last are dates\n",
    "    useful_cols = df.columns[9:].to_list() + ['RegionName']\n",
    "    filtered = df[useful_cols]\n",
    "\n",
    "    # Drop rows where 'RegionName' is NaN\n",
    "    filtered = filtered.dropna(subset=['RegionName'])\n",
    "\n",
    "    # Filter rows where 'RegionName' is in the list of NYC zip codes\n",
    "    filtered = filtered[filtered['RegionName'].isin(nyc_zip)]\n",
    "\n",
    "    # Melting the DataFrame\n",
    "    melted_df = filtered.melt(id_vars=['RegionName'], var_name='Date', value_name='Value')\n",
    "\n",
    "    return melted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T23:24:05.792010Z",
     "start_time": "2023-12-07T23:24:05.383975900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdf\\AppData\\Local\\Temp\\ipykernel_20368\\4019357403.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  a = pd.read_csv(\"data/Bedbug_Reporting_20231203.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Building ID', 'Registration ID', 'Borough', 'House Number',\n",
       "       'Street Name', 'Postcode', '# of Dwelling Units',\n",
       "       'Infested Dwelling Unit Count', 'Eradicated Unit Count',\n",
       "       'Re-infested  Dwelling Unit Count', 'Filing Date',\n",
       "       'Filing Period Start Date', 'Filling Period End Date', 'Latitude',\n",
       "       'Longitude', 'Community Board', 'Council District', '2010 Census Tract',\n",
       "       'BIN', 'BBL', 'NTA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(\"data/Bedbug_Reporting_20231203.csv\")\n",
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from typing import List, Set\n",
    "\n",
    "def filter_bedbug(df: pd.DataFrame, column_needed: List[str], nyc_zip: Set[str]) -> gpd.GeoDataFrame:\n",
    "    # Ensure 'Postcode' and 'Filing Date' are in the needed columns\n",
    "    if 'Postcode' not in column_needed or 'Filing Date' not in column_needed:\n",
    "        raise ValueError(\"Required columns 'Postcode' and 'Filing Date' are missing.\")\n",
    "\n",
    "    # Selecting the required columns and drop rows with NaN values\n",
    "    filtered = df[column_needed].dropna()\n",
    "\n",
    "    # Further filter the DataFrame to only include rows where 'Postcode' is in nyc_zip\n",
    "    filtered = filtered[filtered['Postcode'].isin(nyc_zip)]\n",
    "\n",
    "    # Converting 'Filing Date' to datetime\n",
    "    filtered['Filing Date'] = pd.to_datetime(filtered['Filing Date'])\n",
    "\n",
    "    # Define your date range\n",
    "    start_date = pd.to_datetime('01/01/2015')\n",
    "    end_date = pd.to_datetime('09/30/2023')\n",
    "\n",
    "    # Filter the DataFrame for dates within the range\n",
    "    filtered = filtered[(filtered['Filing Date'] >= start_date) & (filtered['Filing Date'] <= end_date)]\n",
    "\n",
    "    # Convert to GeoDataFrame (assuming Latitude and Longitude columns are present)\n",
    "    if 'Latitude' in filtered.columns and 'Longitude' in filtered.columns:\n",
    "        return gpd.GeoDataFrame(filtered, geometry=gpd.points_from_xy(filtered['Longitude'], filtered['Latitude']))\n",
    "    else:\n",
    "        raise ValueError(\"Latitude and Longitude columns are required for GeoDataFrame conversion.\")\n",
    "\n",
    "    # If no Latitude and Longitude, just return the filtered DataFrame\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define your database connection parameters\n",
    "db_connection_string = \"postgresql://postgres:1234@localhost:5432/e4501project\"\n",
    "engine = create_engine(db_connection_string)\n",
    "\n",
    "# Specify the chunk size\n",
    "chunk_size = 100000\n",
    "\n",
    "# Initialize lists to hold processed chunks\n",
    "t311_chunks = []\n",
    "stc_chunks = []\n",
    "zillow_chunks = []\n",
    "bedbug_chunks = []\n",
    "\n",
    "# Process and store chunks for '311_Service_Requests'\n",
    "for chunk in pd.read_csv('data/311_Service_Requests_from_2010_to_Present_20231129.csv', chunksize=chunk_size):\n",
    "    columns_needed = ['Unique Key', 'Created Date', 'Complaint Type', 'Incident Zip', 'Latitude', 'Longitude', 'Location']\n",
    "    processed_chunk = filter_t311(chunk, columns_needed, nyc_zip=nyc_zips)\n",
    "    t311_chunks.append(processed_chunk)\n",
    "geodf_311_data = pd.concat(t311_chunks)\n",
    "# Process and store chunks for 'StreetTreesCensus_TREES'\n",
    "for chunk in pd.read_csv('data/2015StreetTreesCensus_TREES.csv', chunksize=chunk_size):\n",
    "    columns_needed = ['created_at', 'Latitude', 'longitude', 'tree_id', 'zipcode', 'health', 'spc_common']\n",
    "    processed_chunk = filter_stc(chunk, columns_needed, nyc_zip=nyc_zips)\n",
    "    stc_chunks.append(processed_chunk)\n",
    "geodf_tree_data = pd.concat(stc_chunks)\n",
    "# Process and store chunks for 'zillow_rent_data'\n",
    "for chunk in pd.read_csv('data/zillow_rent_data.csv', chunksize=chunk_size):\n",
    "    processed_chunk = filter_zillow(chunk, nyc_zip=nyc_zips)\n",
    "    zillow_chunks.append(processed_chunk)\n",
    "df_zillow_data = pd.concat(zillow_chunks)\n",
    "# Process and store chunks for 'Bedbug_Reporting'\n",
    "for chunk in pd.read_csv('data/Bedbug_Reporting_20231203.csv', chunksize=chunk_size):\n",
    "    columns_needed = ['Building ID', 'Postcode', 'Filing Date', 'Eradicated Unit Count', 'Re-infested  Dwelling Unit Count','Latitude','Longitude']\n",
    "    processed_chunk = filter_bedbug(chunk, columns_needed, nyc_zip=nyc_zips)\n",
    "    bedbug_chunks.append(processed_chunk)\n",
    "df_bedbug_data = pd.concat(bedbug_chunks)\n",
    "\n",
    "# Load shapefile and save to the database\n",
    "geodf_zipcode_data = gpd.read_file('data/nyc_zipcodes/nyc_zipcodes.shp')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 248 entries, 0 to 262\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   248 non-null    object  \n",
      " 1   geometry  248 non-null    geometry\n",
      "dtypes: geometry(1), object(1)\n",
      "memory usage: 5.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about each dataframe\n",
    "geodf_zipcode_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:37.308893700Z",
     "start_time": "2023-12-08T02:21:37.261897300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "  zipcode                                           geometry\n0   11436  POLYGON ((-8216029.470 4965682.769, -8216011.9...\n1   11213  POLYGON ((-8230673.455 4965216.008, -8230392.3...\n2   11212  POLYGON ((-8226837.796 4963911.170, -8226758.2...\n3   11225  POLYGON ((-8232963.912 4963884.338, -8232717.3...\n4   11218  POLYGON ((-8234534.400 4960940.544, -8234516.0...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>zipcode</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11436</td>\n      <td>POLYGON ((-8216029.470 4965682.769, -8216011.9...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11213</td>\n      <td>POLYGON ((-8230673.455 4965216.008, -8230392.3...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11212</td>\n      <td>POLYGON ((-8226837.796 4963911.170, -8226758.2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11225</td>\n      <td>POLYGON ((-8232963.912 4963884.338, -8232717.3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11218</td>\n      <td>POLYGON ((-8234534.400 4960940.544, -8234516.0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:38.321278900Z",
     "start_time": "2023-12-08T02:21:38.302276600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geodf_311_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[154], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgeodf_311_data\u001B[49m\u001B[38;5;241m.\u001B[39minfo()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'geodf_311_data' is not defined"
     ]
    }
   ],
   "source": [
    "geodf_311_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:39.146864100Z",
     "start_time": "2023-12-08T02:21:39.140864600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geodf_311_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[155], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgeodf_311_data\u001B[49m\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'geodf_311_data' is not defined"
     ]
    }
   ],
   "source": [
    "geodf_311_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:40.079429500Z",
     "start_time": "2023-12-08T02:21:40.046430500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geodf_tree_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[156], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgeodf_tree_data\u001B[49m\u001B[38;5;241m.\u001B[39minfo()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'geodf_tree_data' is not defined"
     ]
    }
   ],
   "source": [
    "geodf_tree_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:40.915505700Z",
     "start_time": "2023-12-08T02:21:40.911503200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geodf_tree_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[157], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgeodf_tree_data\u001B[49m\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'geodf_tree_data' is not defined"
     ]
    }
   ],
   "source": [
    "geodf_tree_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:41.529717300Z",
     "start_time": "2023-12-08T02:21:41.498710200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_zillow_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[158], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdf_zillow_data\u001B[49m\u001B[38;5;241m.\u001B[39minfo()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_zillow_data' is not defined"
     ]
    }
   ],
   "source": [
    "df_zillow_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:42.177254200Z",
     "start_time": "2023-12-08T02:21:42.154254Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_zillow_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[159], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdf_zillow_data\u001B[49m\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_zillow_data' is not defined"
     ]
    }
   ],
   "source": [
    "df_zillow_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T02:21:43.204998100Z",
     "start_time": "2023-12-08T02:21:43.198999300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-12-08T07:04:46.347589400Z",
     "start_time": "2023-12-08T07:04:45.923557100Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_t311.to_postgis('t311', engine, if_exists='append', index=False)\n",
    "# df_stc.to_postgis('stc', engine, if_exists='append', index=False)\n",
    "# df_zillow.to_postgis('zillow', engine, if_exists='append', index=False)\n",
    "# df_bedbug.to_postgis('Bedbug', engine, if_exists='append', index=False)\n",
    "geodf_zipcode_data.to_postgis('nyc_shape', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Storing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!createdb FINAL_PROJECT_4511\n",
    "!psql --dbname FINAL_PROJECT_4511 -c 'CREATE EXTENSION if NOT EXISTS postgis;'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T06:21:02.245394800Z",
     "start_time": "2023-12-08T06:20:55.575453700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def setup_new_postgis_database(username, db_name):\n",
    "    raise NotImplementedError()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "setup_new_postgis_database(DB_USER, DB_NAME)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating Tables\n",
    "\n",
    "\n",
    "These are just a couple of options to creating your tables; you can use one or the other, a different method, or a combination."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "engine = db.create_engine(DB_URL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Option 1: SQL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the SQL statements to create your 4 tables\n",
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc_shape (\n",
    "  \"zipcode\" float8 PRIMARY KEY,\n",
    "  \"geometry\" geometry(POLYGON, 3857)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS t311 (\n",
    "    \"Unique Key\" int8 PRIMARY KEY,\n",
    "    \"Created Date\" timestamp(6),\n",
    "    \"Complaint Type\" text COLLATE \"pg_catalog\".\"default\",\n",
    "    \"Incident Zip\" float8,\n",
    "    \"Latitude\" float8,\n",
    "    \"Longitude\" float8,\n",
    "    \"Location\" text COLLATE \"pg_catalog\".\"default\",\n",
    "    \"geometry\" geometry(POINT, 3857)\n",
    ")\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stc (\n",
    "    \"created_at\" timestamp(6),\n",
    "    \"Latitude\" float8,\n",
    "    \"longitude\" float8,\n",
    "    \"tree_id\" int8 PRIMARY KEY,\n",
    "    \"zipcode\" int8,\n",
    "    \"health\" text COLLATE \"pg_catalog\".\"default\",\n",
    "    \"spc_common\" text COLLATE \"pg_catalog\".\"default\",\n",
    "    \"geometry\" geometry(POINT, 3857)\n",
    "\"\"\"\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS zillow (\n",
    "    \"RegionName\" int8 PRIMARY KEY,\n",
    "    \"Date\" text COLLATE \"pg_catalog\".\"default\",\n",
    "    \"Value\" float8\n",
    "\"\"\"\n",
    "\n",
    "BEDBUG_SCHEMA = '''\n",
    "CREATE TABLE IF NOT EXISTS Bedbug (\n",
    "  \"Building ID\" int8 PRIMARY KEY,\n",
    "  \"Postcode\" float8,\n",
    "  \"Filing Date\" timestamp(6),\n",
    "  \"Eradicated Unit Count\" float8,\n",
    "  \"Re-infested  Dwelling Unit Count\" float8,\n",
    "  \"Latitude\" float8,\n",
    "  \"Longitude\" float8,\n",
    "  \"geometry\" geometry(POINT)\n",
    ")'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T07:21:31.149347800Z",
     "start_time": "2023-12-08T07:21:31.135839800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open('schema.sql', \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "    f.write(NYC_311_SCHEMA)\n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "    f.write(ZILLOW_SCHEMA)\n",
    "    f.write(BEDBUG_SCHEMA)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T07:21:43.675404900Z",
     "start_time": "2023-12-08T07:21:43.665404900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If using SQL (as opposed to SQLAlchemy), execute the schema files to create tables\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
